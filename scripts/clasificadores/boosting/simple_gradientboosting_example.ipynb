{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11f7384",
   "metadata": {},
   "source": [
    "# Implementación Manual de Gradient Boosting para Clasificación Binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210562fa",
   "metadata": {},
   "source": [
    "### Carga del dataset Pima Indian Diabetes\n",
    "\n",
    "En esta celda, se carga el dataset \"Pima Indian Diabetes\" desde un archivo CSV ubicado en el directorio especificado. El dataset contiene información sobre pacientes, incluyendo características como número de embarazos, nivel de glucosa, presión arterial, grosor de la piel, nivel de insulina, índice de masa corporal (BMI), función de pedigrí de diabetes y edad. Además, incluye la variable objetivo $\\text{Outcome}$, que indica si el paciente tiene diabetes ($1$) o no ($0$).\n",
    "\n",
    "Formalmente, el dataset puede representarse como una matriz $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, donde $n$ es el número de muestras y $d$ el número de características, y un vector objetivo $\\mathbf{y} \\in \\{0,1\\}^n$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
    "x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Se realiza la separación de las características ($\\mathbf{X}$) y la variable objetivo ($\\mathbf{y}$) para su uso en el modelo de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc9fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# 1. Cargar dataset Pima Indian Diabetes\n",
    "df = pd.read_csv(\"../../../datasets/pima_indian_diabetes_dataset/cleaned_dataset.csv\")  # Asegúrate de tener este archivo en tu directorio\n",
    "X = df.drop(columns=[\"Outcome\"])\n",
    "y = df[\"Outcome\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64eedc8",
   "metadata": {},
   "source": [
    "### División de los datos en entrenamiento y prueba\n",
    "\n",
    "En esta celda, se realiza la división del dataset en conjuntos de entrenamiento ($\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}$) y prueba ($\\mathbf{X}_{\\text{test}}, \\mathbf{y}_{\\text{test}}$). Esto permite evaluar el rendimiento del modelo en datos no vistos durante el entrenamiento. La proporción de división es del 80\\% para entrenamiento y 20\\% para prueba, utilizando una semilla aleatoria ($\\text{random\\_state}=42$) para garantizar la reproducibilidad de los resultados.\n",
    "\n",
    "Formalmente:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}, \\mathbf{y} \\longrightarrow (\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}), (\\mathbf{X}_{\\text{test}}, \\mathbf{y}_{\\text{test}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c5e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15acfe2",
   "metadata": {},
   "source": [
    "### Parámetros del modelo Gradient Boosting manual\n",
    "\n",
    "1. **$n_{\\text{rounds}}$**: Número de rondas de entrenamiento. Representa la cantidad de iteraciones en las que se ajustan árboles de decisión para mejorar las predicciones. En este caso, su valor es $10$.\n",
    "\n",
    "2. **$\\eta$ (learning rate)**: Tasa de aprendizaje utilizada para escalar las actualizaciones de los árboles en cada iteración. Controla la contribución de cada árbol al modelo final. Su valor es $1.0$.\n",
    "\n",
    "En resumen:\n",
    "\n",
    "- $n_{\\text{rounds}} = 10$\n",
    "- $\\eta = 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Parámetros de Gradient Boosting\n",
    "n_rounds = 10\n",
    "learning_rate = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aeedc7",
   "metadata": {},
   "source": [
    "### Inicialización con log-odds y configuración inicial del modelo\n",
    "\n",
    "En esta celda, se realiza la inicialización del modelo de Gradient Boosting manual. Se calcula el valor inicial de predicción constante ($y_{\\text{mean}}$) utilizando el log-odds de la proporción de la variable objetivo en el conjunto de entrenamiento ($\\mathbf{y}_{\\text{train}}$). Este valor representa la predicción inicial antes de ajustar cualquier árbol.\n",
    "\n",
    "La inicialización se realiza como:\n",
    "\n",
    "$$\n",
    "y_{\\text{mean}} = \\log\\left(\\frac{\\bar{p}}{1-\\bar{p}}\\right), \\quad \\bar{p} = \\frac{1}{N} \\sum_{i=1}^N y_i\n",
    "$$\n",
    "\n",
    "donde $\\bar{p}$ es la proporción de positivos en el conjunto de entrenamiento.\n",
    "\n",
    "Además, se inicializan las predicciones para el conjunto de entrenamiento ($\\hat{y}_{\\text{train}}$) con el valor constante $y_{\\text{mean}}$. También se crea una lista vacía para almacenar los árboles de decisión que se ajustarán en cada iteración del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f153ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Inicialización con log-odds (predicción constante)\n",
    "y_mean = np.log(y_train.mean() / (1 - y_train.mean()))\n",
    "pred_train = np.full(y_train.shape, y_mean)\n",
    "trees = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30a612",
   "metadata": {},
   "source": [
    "### Entrenamiento iterativo del modelo Gradient Boosting manual\n",
    "\n",
    "En esta celda, se realiza el entrenamiento iterativo del modelo Gradient Boosting manual. Durante cada ronda de entrenamiento:\n",
    "\n",
    "1. Se calculan las probabilidades actuales ($p_i$) utilizando la función sigmoide sobre las predicciones acumuladas ($\\hat{y}_i$):\n",
    "   $$\n",
    "   p_i = \\sigma(\\hat{y}_i) = \\frac{1}{1 + e^{-\\hat{y}_i}}\n",
    "   $$\n",
    "2. Se calcula el gradiente negativo ($g_i$), que representa los residuos entre los valores reales ($y_i$) y las probabilidades actuales ($p_i$):\n",
    "   $$\n",
    "   g_i = y_i - p_i\n",
    "   $$\n",
    "3. Se ajusta un árbol de decisión $h_t(\\mathbf{x})$ con profundidad máxima de 1 utilizando el gradiente como objetivo.\n",
    "4. Se actualizan las predicciones acumuladas:\n",
    "   $$\n",
    "   \\hat{y}_i \\leftarrow \\hat{y}_i + \\eta \\cdot h_t(\\mathbf{x}_i)\n",
    "   $$\n",
    "5. Se almacena el árbol ajustado en la lista para su uso en futuras predicciones.\n",
    "\n",
    "Al final de cada ronda, se calcula el AUC acumulado en el conjunto de entrenamiento para evaluar el rendimiento del modelo. Este proceso se repite por el número de rondas especificado ($n_{\\text{rounds}}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e6dd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ronda 1: AUC acumulado (train) = 0.7554\n",
      "Ronda 2: AUC acumulado (train) = 0.7861\n",
      "Ronda 3: AUC acumulado (train) = 0.7861\n",
      "Ronda 4: AUC acumulado (train) = 0.8382\n",
      "Ronda 5: AUC acumulado (train) = 0.8412\n",
      "Ronda 6: AUC acumulado (train) = 0.8607\n",
      "Ronda 7: AUC acumulado (train) = 0.8642\n",
      "Ronda 8: AUC acumulado (train) = 0.8666\n",
      "Ronda 9: AUC acumulado (train) = 0.8709\n",
      "Ronda 10: AUC acumulado (train) = 0.8765\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Entrenamiento iterativo\n",
    "for t in range(n_rounds):\n",
    "    p = 1 / (1 + np.exp(-pred_train))  # Probabilidad actual\n",
    "    gradient = y_train - p             # Gradiente negativo (residuo)\n",
    "\n",
    "    tree = DecisionTreeRegressor(max_depth=1, random_state=42)\n",
    "    tree.fit(X_train, gradient)\n",
    "\n",
    "    update = tree.predict(X_train)\n",
    "    pred_train += learning_rate * update\n",
    "    trees.append(tree)\n",
    "\n",
    "    final_preds = 1 / (1 + np.exp(-pred_train))\n",
    "    auc = roc_auc_score(y_train, final_preds)\n",
    "    print(f\"Ronda {t+1}: AUC acumulado (train) = {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0bfa0",
   "metadata": {},
   "source": [
    "### Predicción final en el conjunto de prueba\n",
    "\n",
    "En esta celda, se realiza la predicción final utilizando el modelo de Gradient Boosting manual en el conjunto de prueba ($\\mathbf{X}_{\\text{test}}$). \n",
    "\n",
    "1. Se inicializan las predicciones ($\\hat{y}_{\\text{test}}$) con el valor constante calculado previamente ($y_{\\text{mean}}$).\n",
    "2. Se actualizan las predicciones acumuladas sumando el producto de la tasa de aprendizaje ($\\eta$) y las predicciones de cada árbol ajustado durante el entrenamiento:\n",
    "   $$\n",
    "   \\hat{y}_{\\text{test}} \\leftarrow \\hat{y}_{\\text{test}} + \\eta \\cdot h_t(\\mathbf{x}_{\\text{test}})\n",
    "   $$\n",
    "3. Se calculan las probabilidades finales aplicando la función sigmoide:\n",
    "   $$\n",
    "   p_{\\text{final}} = \\sigma(\\hat{y}_{\\text{test}}) = \\frac{1}{1 + e^{-\\hat{y}_{\\text{test}}}}\n",
    "   $$\n",
    "4. Se determinan las clases finales utilizando un umbral de 0.5:\n",
    "   $$\n",
    "   \\hat{y}_{\\text{final}} = \\begin{cases} 1 & \\text{si } p_{\\text{final}} \\geq 0.5 \\\\ 0 & \\text{en otro caso} \\end{cases}\n",
    "   $$\n",
    "\n",
    "Estas predicciones finales se utilizarán para evaluar el rendimiento del modelo en el conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f9a35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Predicción final en test\n",
    "pred_test = np.full(y_test.shape, y_mean)\n",
    "for tree in trees:\n",
    "    pred_test += learning_rate * tree.predict(X_test)\n",
    "\n",
    "final_probs = 1 / (1 + np.exp(-pred_test))  # Probabilidad\n",
    "final_classes = (final_probs >= 0.5).astype(int)  # Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2bf5bd",
   "metadata": {},
   "source": [
    "### Evaluación del modelo Gradient Boosting manual\n",
    "\n",
    "En esta celda, se evalúa el rendimiento del modelo Gradient Boosting manual en el conjunto de prueba ($\\mathbf{X}_{\\text{test}}$) utilizando las métricas de evaluación:\n",
    "\n",
    "1. **AUC (Area Under the Curve)**: Representa la capacidad del modelo para distinguir entre las clases. Un valor más cercano a 1 indica un mejor rendimiento. Se calcula como el área bajo la curva ROC.\n",
    "2. **Accuracy**: Proporción de predicciones correctas realizadas por el modelo en el conjunto de prueba:\n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(y_i = \\hat{y}_i)\n",
    "   $$\n",
    "   donde $\\mathbb{I}$ es la función indicadora.\n",
    "3. **Matriz de confusión**: Muestra el número de verdaderos positivos (TP), verdaderos negativos (TN), falsos positivos (FP) y falsos negativos (FN), proporcionando una visión detallada del rendimiento del modelo:\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   \\text{TN} & \\text{FP} \\\\\n",
    "   \\text{FN} & \\text{TP}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "Los resultados obtenidos se imprimen para analizar la efectividad del modelo en la tarea de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd0ee69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC en test (Gradient Boosting manual): 0.8333\n",
      "Accuracy en test: 0.7215\n",
      "Matriz de confusión:\n",
      "[[46  0]\n",
      " [22 11]]\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluación\n",
    "final_auc = roc_auc_score(y_test, final_probs)\n",
    "accuracy = accuracy_score(y_test, final_classes)\n",
    "cm = confusion_matrix(y_test, final_classes)\n",
    "\n",
    "print(f\"\\nAUC en test (Gradient Boosting manual): {final_auc:.4f}\")\n",
    "print(f\"Accuracy en test: {accuracy:.4f}\")\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
