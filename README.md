## 1MP215 - Topicos Especiales II (Machine Learning)

### Descripción General
- **Duración**: 13 semanas (12 semanas de instrucción + 1 semana de proyecto final)
- **Horario Semanal**: 5 horas/semana (3h lunes, 2h viernes)
- **Formato**: Basado en proyectos con tareas semanales
- **Trabajo en Grupo**: Los estudiantes trabajarán en grupos de 2 para todas las tareas y proyectos.

### Objetivos del Curso

Al finalizar el curso, los estudiantes serán capaces de:
- Comprender los conceptos fundamentales y las aplicaciones de machine learning y deep learning.
- Identificar y diferenciar los principales tipos de problemas y algoritmos de aprendizaje supervisado y no supervisado.
- Implementar y evaluar modelos de machine learning utilizando Python y bibliotecas relevantes.
- Aplicar técnicas de preprocesamiento, selección de características y reducción de dimensionalidad.
- Analizar y comparar el desempeño de diferentes modelos y métricas de evaluación.
- Desarrollar proyectos prácticos en equipo, integrando conocimientos teóricos y habilidades técnicas.
- Utilizar herramientas modernas de desarrollo, control de versiones y plataformas colaborativas.

### Justificación del Curso

El avance acelerado de la inteligencia artificial y el machine learning está transformando múltiples sectores, desde la industria y la salud hasta la educación y los servicios. Comprender los fundamentos y aplicaciones de estas tecnologías es esencial para los profesionales modernos, independientemente de su área de especialización. Este curso proporciona una base sólida en los principios y herramientas de machine learning y deep learning, permitiendo a los estudiantes analizar datos, construir modelos predictivos y resolver problemas reales. Además, fomenta el trabajo colaborativo y el uso de plataformas y herramientas actuales, preparando a los participantes para enfrentar los desafíos tecnológicos del presente y futuro.

### Requisitos Previos
- **Conocimientos Matemáticos**:
  - Se recomienda una comprensión básica de álgebra lineal, cálculo, probabilidad y estadística.
- **Habilidades de Programación**:
  - Familiaridad con Python y bibliotecas como NumPy y scikit-learn es útil.
  - Las tareas y proyectos requerirán scripts en Python.
- **Conocimientos Informáticos**:
  - Usar algoritmos de machine learning requiere cierto nivel de programación.
  - El curso está diseñado para ser accesible a una amplia audiencia, con material de apoyo para ayudar a los estudiantes a ponerse al día.
  - Los estudiantes con diversos antecedentes en matemáticas y programación pueden tener éxito colaborando en equipos.
- **Nota Importante**:
  - Este no es un curso de programación; tu código no será calificado.
  - Sin embargo, trabajar con datos e interpretar resultados será crítico, y los resultados de tu código serán evaluados.

### Herramientas y Plataformas
- **Lenguaje de Programación**: Python
- **Bibliotecas**: NumPy, pandas, Scikit-learn, PyTorch
- **Plataformas**: Jupyter Notebook, Google Colab, Kaggle
- **Control de Versiones**: Git, GitHub

### Software Requerido
- **Editor de Código**:
  - [Visual Studio Code (VS Code)](https://code.visualstudio.com/) - Recomendado.
  - [PyCharm Community Edition](https://www.jetbrains.com/pycharm/) - Opcional.
- **Python**:
  - Versión 3.8 o superior.
- **Gestor de Paquetes**:
  - [pip](https://pip.pypa.io/en/stable/) o [conda](https://docs.conda.io/en/latest/).
- **Otros**:
  - Navegador web para acceder a Google Colab o Kaggle.

### Rúbricas y Criterios de Evaluación
- **Escala de Calificaciones**:
  - A: 91–100
  - B: 81–90
  - C: 71–80
  - D: 61–70
  - F: Menos de 61
- **Rúbrica de Evaluación**:
  - **Asignaciones Semanales (10 en total)**: 30%
  - **Proyectos (3 en total)**: 30%
  - **Proyecto Final**: 40%
- **Evaluación**:
  - Tareas Semanales: Evaluadas por corrección, claridad y completitud.
  - Proyectos: Evaluados por innovación, implementación y presentación.
  - Proyecto Final: Evaluado por originalidad, profundidad técnica e impacto general.

### Integridad Académica y Política de Colaboración
- **Directrices de Colaboración**:
  - Se anima a los estudiantes a colaborar dentro de sus grupos asignados de 2 personas.
  - Se permiten discusiones con otros grupos, pero no deben implicar compartir código o soluciones.
- **Herramientas Permitidas**:
  - Se fomenta el uso de herramientas de IA generativa (por ejemplo, ChatGPT, Copilot) para estudiar y desarrollar proyectos.
  - Se permite el uso de bibliotecas y frameworks externos si se citan adecuadamente.
- **Herramientas Restringidas**:
  - Está prohibido copiar directamente soluciones de fuentes externas (por ejemplo, repositorios de GitHub).
- **Originalidad del Código**:
  - Todo el código enviado será revisado por originalidad utilizando herramientas automatizadas.
  - El plagio o la similitud excesiva con fuentes externas resultará en sanciones.
- **Uso de inteligencia artificial generativa (como ChatGPT)**:
  - Este curso promueve el uso de IA generativa para el aprendizaje y la mejora de los resultados de los proyectos.

### Definiciones
- **Tarea**: Una tarea corta diseñada para completarse en unas pocas horas, enfocada en conceptos o habilidades específicas.
- **Proyecto**: Una tarea más extensa que requiere unos días para completarse, integrando múltiples conceptos y aplicaciones prácticas.
- **Proyecto Final**: Una tarea integral y desafiante diseñada para tomar hasta una semana, mostrando el dominio del material del curso.

---

### Desglose Semanal
---

#### Semanas 1–8: Fundamentos de Machine Learning

##### Semana 1
- **Lunes (12 de mayo)**:
  - Temas:
    - Introducción a Machine Learning
      - Definición y aplicaciones de machine learning en el mundo real.
      - Breve historia y evolución de machine learning.
    - Regresión vs clasificación
      - Diferencias clave y ejemplos de problemas de regresión y clasificación.
      - Casos de uso para cada tipo de problema.
    - Aprendizaje supervisado vs no supervisado
      - Descripción general del aprendizaje supervisado con datos etiquetados.
      - Introducción al aprendizaje no supervisado y técnicas de agrupamiento.
      - Ejemplos de aprendizaje supervisado y no supervisado en la práctica.
  - Tareas: Ninguna
- **Viernes (16 de mayo)**:
  - Temas:
    - K-Nearest Neighbors (KNN): conceptos, métricas de distancia, demostración con MNIST
      - Explicación del algoritmo KNN y su principio de funcionamiento.
      - Discusión sobre métricas de distancia (por ejemplo, Euclidiana, Manhattan).
      - Demostración práctica utilizando el conjunto de datos MNIST.
    - Normalización y estandarización
      - Importancia de la escala de características en machine learning.
      - Diferencias entre normalización y estandarización.
      - Ejemplos prácticos de aplicación de técnicas de escalado.
  - Tareas: Ninguna

##### Semana 2
- **Lunes (19 de mayo)**:
  - Temas:
    - Correlación de Pearson: comprensión de relaciones entre variables.
    - Introducción a modelos lineales.
    - Regresión lineal y sus extensiones.
    - Regresión logística.
    - Técnicas de regularización (L1, L2).
    - Clasificador Ridge como ejemplo.
  - Tareas:
    - Asignación 1 asignada.
    - Proyecto 1 asignado (entrega: viernes de la semana 5).
- **Viernes (23 de mayo)**:
  - Temas:
    - Balanceo de clases:
      - Importancia del balanceo en problemas de clasificación.
      - Consecuencias del desbalance en el desempeño del modelo.
      - Estrategias comunes:
        - Sobremuestreo de clases minoritarias.
        - Submuestreo de clases mayoritarias.
        - Generación sintética de ejemplos (mención de técnicas como SMOTE).
      - Consideraciones al aplicar estas técnicas: riesgo de sobreajuste y pérdida de información.
    - Evaluación de métricas:
      - Clasificación:
        - Matriz de confusión: Verdaderos Positivos (TP), Falsos Positivos (FP), Verdaderos Negativos (TN), Falsos Negativos (FN).
        - Métricas derivadas:
          - Precisión (Precision)
          - Exhaustividad (Recall)
          - Puntaje F1 (F1-score)
          - Exactitud global (Accuracy)
          - Área bajo la curva ROC (AUC-ROC)
      - Regresión:
        - Error Cuadrático Medio (MSE)
        - Raíz del Error Cuadrático Medio (RMSE)
        - Error Absoluto Medio (MAE)
        - Coeficiente de Determinación (R²)
      - Comparación entre métricas según la sensibilidad a errores grandes o pequeños.
      - Recomendaciones para seleccionar métricas según el contexto del problema.
  - Tareas: Ninguna

##### Semana 3
- **Lunes (26 de mayo)**:
  - Temas:
    - Introducción a modelos probabilísticos.
    - Teorema de Bayes y sus aplicaciones.
    - Distribuciones de datos.
    - Distribución normal.
  - Tareas:
    - Asignación 2 asignada.
- **Viernes (30 de mayo)**:
  - Temas:
    - Modelos Naive Bayes.
    - Naive Bayes Gaussiano (GNB).
    - Naive Bayes Multinomial (MNB).
    - Naive Bayes Categórico (CNB).
    - Naive Bayes Bernoulli (BNB).
  - Tareas:
    - Entrega de la Asignación 1.

##### Semana 4
- **Lunes (2 de junio)**:
  - Temas:
    - Árboles de decisión.
    - Entropía.
    - Índice Gini.
    - Sobreajuste.
    - Manejo de datos categóricos y numéricos en árboles de decisión.
    - Visualización de árboles de decisión.
  - Tareas:
    - Asignación 3 asignada.
- **Viernes (6 de junio)**:
  - Temas:
    - Importancia de características.
    - Estrategias de poda.
    - Técnicas avanzadas de poda.
    - Comparación de algoritmos de árboles de decisión.
  - Tareas:
    - Entrega de la Asignación 2.

##### Semana 5
- **Lunes (9 de junio)**:
  - Temas:
    - Máquinas de Soporte Vectorial (SVM).
      - Introducción a SVM.
      - Concepto de hiperplanos y límites de decisión.
      - SVM lineal vs no lineal.
      - Funciones kernel (lineal, polinomial, RBF).
      - Margen suave vs margen duro.
    - Kernels y márgenes.
      - Ejemplos prácticos del uso de kernels.
      - Visualización de límites de decisión.
  - Tareas:
    - Asignación 4 asignada.
- **Viernes (13 de junio)**:
  - Temas:
    - GridSearchCV.
      - Automatización de ajuste de hiperparámetros.
      - Ejemplos prácticos con scikit-learn.
    - Validación cruzada k-fold.
      - Técnicas de validación cruzada.
      - Evitar sobreajuste con k-fold CV.
    - Optimización de hiperparámetros.
      - Técnicas: búsqueda aleatoria vs búsqueda en cuadrícula.
      - Introducción a la optimización bayesiana.
  - Tareas:
    - Entrega de la Asignación 3.
    - Entrega del Proyecto 1.
    - Proyecto 2 asignado (entrega: viernes de la semana 8).

##### Semana 6
- **Lunes (16 de junio)**:
  - Temas:
    - Random Forests
      - Introducción a métodos de ensamble.
      - Cómo los Random Forests combinan múltiples árboles de decisión.
      - Importancia de características en Random Forests.
      - Manejo del sobreajuste con Random Forests.
    - Bagging
      - Concepto de bootstrap aggregation.
      - Ventajas del bagging para reducir la varianza.
      - Comparación de bagging con boosting.
  - Tareas:
    - Asignación 5 asignada.
- **Viernes (20 de junio)**:
  - Temas:
    - Error out-of-bag
      - Explicación y relevancia en Random Forests.
    - Técnicas avanzadas de bagging
      - Método de subespacio aleatorio.
      - Pasting (sin reemplazo).
    - Introducción a Extra Trees (Árboles Extremadamente Aleatorizados)
      - Diferencias entre Extra Trees y Random Forests.
      - Casos de uso y ventajas.
  - Tareas:
    - Entrega de la Asignación 4.

##### Semana 7
- **Lunes (23 de junio)**:
  - Temas:
    - Boosting
      - Concepto de boosting y aprendizaje iterativo.
      - Adaboost
        - Cómo Adaboost combina clasificadores débiles.
        - Ajuste de pesos para muestras mal clasificadas.
      - Gradient Boosting
        - Concepto de descenso de gradiente en boosting.
        - Diferencias entre Adaboost y Gradient Boosting.
      - XGBoost (eXtreme Gradient Boosting)
        - Introducción a XGBoost y sus ventajas.
        - Aplicaciones prácticas de XGBoost.
  - Tareas:
    - Asignación 6 asignada.
- **Viernes (27 de junio)**:
  - Temas:
    - Comparación de métodos de ensamble
      - Bagging vs Boosting.
      - Fortalezas y debilidades de cada enfoque.
    - Stacking
      - Concepto de apilamiento de modelos.
      - Meta-modelos y su función en stacking.
    - Ejemplos prácticos de métodos de ensamble
      - Casos de uso en conjuntos de datos reales.
  - Tareas:
    - Entrega de la Asignación 5.

##### Semana 8
- **Lunes (30 de junio)**:
  - Temas:
    - Clustering
      - Introducción al clustering y sus aplicaciones.
      - K-means clustering
        - Descripción del algoritmo.
        - Selección del número de clusters (método del codo, puntuación de silueta).
        - Ejemplos prácticos con scikit-learn.
      - DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
        - Concepto de clustering basado en densidad.
        - Ventajas sobre k-means.
        - Casos de uso y limitaciones.
  - Tareas:
    - Asignación 7 asignada.
- **Viernes (4 de julio)**:
  - Temas:
    - PCA (Análisis de Componentes Principales)
      - Concepto de reducción de dimensionalidad.
      - Valores y vectores propios.
      - Visualización de datos de alta dimensión en 2D/3D.
      - Ejemplos prácticos con scikit-learn.
    - Feature Engineering
      - Importancia de la selección y extracción de características.
      - Técnicas para crear nuevas características.
    - Reducción de dimensionalidad
      - Comparación de PCA con otras técnicas.
      - Casos de uso de reducción de dimensionalidad en machine learning.
  - Tareas:
    - Entrega de la Asignación 6.
    - Entrega del Proyecto 2.
    - Proyecto 3 asignado (entrega: viernes de la semana 11).

---

#### Semanas 9–12: Enfoque en Deep Learning

##### Semana 9
- **Lunes (7 de julio)**:
  - Temas:
    - Introducción a Deep Learning
      - Definición y diferencias clave respecto a machine learning tradicional.
      - Aplicaciones de deep learning en distintos dominios.
    - Perceptrones
      - Concepto de perceptrón como bloque básico de redes neuronales.
      - Representación matemática y funciones de activación.
    - Funciones de activación
      - Repaso de funciones comunes (sigmoide, ReLU, tanh).
      - Importancia de la no linealidad en redes neuronales.
  - Tareas:
    - Asignación 8 asignada.
- **Viernes (11 de julio)**:
  - Temas:
    - Redes neuronales feedforward
      - Estructura de redes neuronales feedforward.
      - Propagación hacia adelante y actualización de pesos.
    - Funciones de pérdida
      - Explicación de funciones de pérdida (MSE, entropía cruzada).
      - Rol de la función de pérdida en el entrenamiento.
  - Tareas:
    - Entrega de la Asignación 7.

##### Semana 10
- **Lunes (14 de julio)**:
  - Temas:
    - Backpropagation
      - Explicación detallada del algoritmo de retropropagación.
      - Rol de los gradientes y la regla de la cadena en la actualización de pesos.
    - Regularización (dropout, batch norm)
      - Técnicas para evitar el sobreajuste en redes neuronales.
      - Explicación de dropout y batch normalization con ejemplos.
  - Tareas:
    - Asignación 9 asignada.
- **Viernes (18 de julio)**:
  - Temas:
    - Ajuste de hiperparámetros
      - Importancia de ajustar hiperparámetros para el rendimiento del modelo.
      - Hiperparámetros comunes en deep learning (learning rate, batch size).
    - Schedulers de tasa de aprendizaje
      - Técnicas para ajustar la tasa de aprendizaje durante el entrenamiento.
      - Ejemplos de step decay, exponential decay y tasas de aprendizaje cíclicas.
  - Tareas:
    - Entrega de la Asignación 8.

##### Semana 11
- **Lunes (21 de julio)**:
  - Temas:
    - CNNs: convolución, filtros, pooling
      - Introducción a redes neuronales convolucionales (CNNs).
      - Explicación de capas convolucionales, filtros y mapas de características.
      - Técnicas de pooling (max pooling, average pooling) y su rol en la reducción de dimensionalidad.
  - Tareas:
    - Asignación 10 asignada.
- **Viernes (25 de julio)**:
  - Temas:
    - CNN para clasificación de imágenes
      - Construcción de una CNN para tareas de clasificación de imágenes.
      - Ejemplo práctico usando un dataset (CIFAR-10 o MNIST).
      - Discusión sobre transfer learning para clasificación de imágenes.
  - Tareas:
    - Entrega de la Asignación 9.
    - Entrega del Proyecto 3.

##### Semana 12
- **Lunes (28 de julio)**:
  - Temas:
    - Transfer learning con redes preentrenadas
      - Concepto de transfer learning y sus ventajas.
      - Ejemplos de modelos preentrenados populares (VGG, ResNet).
      - Fine-tuning de modelos preentrenados para tareas específicas.
  - Tareas:
    - Ninguna.
- **Viernes (1 de agosto)**:
  - Temas:
    - Test-time augmentation
      - Técnicas para mejorar el rendimiento del modelo durante la inferencia.
      - Ejemplos de aumentaciones (flip, crop).
    - Análisis de errores
      - Identificación y abordaje de fuentes comunes de error en modelos de deep learning.
      - Estrategias para mejorar el rendimiento basadas en el análisis de errores.
  - Tareas:
    - Entrega de la Asignación 10.

---

#### Semana 13: Proyecto Final

##### Semana de Presentación Final
- **Lunes**:
  - Temas:
    - Presentaciones del proyecto final

---

### Perfil del Docente

**Antony Garcia**  
- Ingeniero Electromecánico egresado de la Universidad Tecnológica de Panamá (UTP).  
- M. Sc. en Ingeniería Eléctrica con especialización en Potencia Eléctrica (UTP).  
- Estudiante de doctorado en Worcester Polytechnic Institute (WPI)
    - Especialización en Data Science y Computer Science
    - Actualmente trabajando en disertación doctoral en aplicaciones de Inteligencia Artificial en healthcare

**Contacto**  
- Correo electrónico: antony.garcia@utp.ac.pa  
- Disponible para consultas por correo o a través del grupo de Teams.
