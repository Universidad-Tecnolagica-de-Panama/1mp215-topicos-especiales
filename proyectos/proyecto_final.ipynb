{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563bc726",
   "metadata": {},
   "source": [
    "## Proyecto Final de Curso\n",
    "### Clasificación de Defectos en Infraestructuras con Modelos Preentrenados\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este proyecto tiene como finalidad que los estudiantes se familiaricen con el uso de **modelos preentrenados de deep learning** para tareas de **clasificación de imágenes** aplicadas en ingeniería.\n",
    "\n",
    "Utilizando el conjunto de datos [CODEBRIM](https://zenodo.org/record/2620293), que contiene imágenes etiquetadas de defectos en puentes, los estudiantes deberán:\n",
    "\n",
    "1. **Explorar y preprocesar el dataset** para tareas de clasificación.\n",
    "2. **Aplicar transferencia de aprendizaje** utilizando **modelos preentrenados de PyTorch**.\n",
    "3. **Entrenar al menos tres modelos distintos** (por ejemplo: ResNet, EfficientNet, DenseNet).\n",
    "4. **Evaluar y comparar el rendimiento** de los modelos mediante métricas apropiadas (accuracy, precision, recall, F1-score, matriz de confusión, ROC AUC).\n",
    "\n",
    "El objetivo principal es que comprendan cómo aplicar arquitecturas modernas a problemas reales de clasificación visual en el contexto de la ingeniería.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62111500",
   "metadata": {},
   "source": [
    "## Descripción del Dataset\n",
    "\n",
    "El dataset utilizado en este proyecto es **CODEBRIM (Concrete Defect BRidge IMage Dataset)**, diseñado para tareas de clasificación de imágenes con aplicación en infraestructura civil.\n",
    "\n",
    "### Descarga\n",
    "\n",
    "El dataset puede descargarse desde el siguiente enlace:\n",
    "\n",
    "[CODEBRIM_classification_dataset.zip (7.9 GB)](https://zenodo.org/records/2620293/files/CODEBRIM_classification_dataset.zip?download=1)\n",
    "\n",
    "### Contenido\n",
    "\n",
    "El conjunto de datos está organizado en tres particiones predefinidas: entrenamiento, validación y prueba.\n",
    "\n",
    "| Partición     | Imágenes con Defectos | Imágenes sin Defectos |\n",
    "|---------------|------------------------|------------------------|\n",
    "| Entrenamiento | 4,297                  | 2,186                  |\n",
    "| Validación    | 467                    | 150                    |\n",
    "| Prueba        | 483                    | 150                    |\n",
    "\n",
    "### Tarea\n",
    "\n",
    "Este proyecto se enfoca en una tarea de **clasificación binaria**:\n",
    "- **Clase 1**: Imagen con algún tipo de **defecto estructural**\n",
    "- **Clase 0**: Imagen **sin defectos** (fondo)\n",
    "\n",
    "### Tipos de defectos incluidos\n",
    "\n",
    "Las imágenes con defecto pueden presentar uno o más de los siguientes problemas estructurales:\n",
    "\n",
    "- **Grietas (Cracks)**\n",
    "- **Spalling** (fragmentación del concreto)\n",
    "- **Corrosión**\n",
    "- **Eflorescencia** (depósitos de sal)\n",
    "- **Barras expuestas**\n",
    "\n",
    "Estas clases están anotadas visualmente por expertos y reflejan condiciones reales observadas en infraestructura deteriorada, como puentes de concreto.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e8dc4",
   "metadata": {},
   "source": [
    "## Consideraciones sobre el Preprocesamiento de Imágenes\n",
    "\n",
    "Uno de los principales retos al trabajar con el dataset CODEBRIM es la **gran variabilidad en los tamaños de las imágenes**. La mayoría de las arquitecturas de modelos preentrenados en visión por computadora (por ejemplo, ResNet, EfficientNet, DenseNet) requieren que las imágenes de entrada tengan un tamaño fijo de **224×224×3**, es decir:\n",
    "\n",
    "- **224 píxeles de ancho**\n",
    "- **224 píxeles de alto**\n",
    "- **3 canales de color (RGB)**\n",
    "\n",
    "### Desafío\n",
    "\n",
    "Las imágenes originales del dataset:\n",
    "- Son mucho más grandes (varios cientos o miles de píxeles por lado).\n",
    "- No necesariamente tienen una forma cuadrada.\n",
    "\n",
    "### Solución Sugerida\n",
    "\n",
    "Se recomienda aplicar un **proceso de normalización del tamaño** utilizando **padding y resize**:\n",
    "\n",
    "1. **Padding**: Agregar píxeles negros (valor 0) a los bordes más cortos para transformar la imagen en un formato cuadrado.\n",
    "2. **Resize**: Redimensionar la imagen resultante a 224x224.\n",
    "\n",
    "Este método garantiza compatibilidad con los modelos preentrenados sin distorsionar significativamente las proporciones de los defectos.\n",
    "\n",
    "### Alternativas Avanzadas (Opcionales)\n",
    "\n",
    "Los estudiantes que deseen experimentar con técnicas más sofisticadas pueden explorar:\n",
    "\n",
    "- **Windowing**: División de la imagen en parches más pequeños mediante una ventana deslizante.\n",
    "- **Cropping inteligente**: Recortes automáticos de regiones de interés centradas en los defectos.\n",
    "- **Técnicas adaptativas**: Uso de algoritmos que identifiquen zonas relevantes antes de ajustar el tamaño.\n",
    "\n",
    "---\n",
    "\n",
    "> **Nota Importante de Evaluación**:\n",
    ">\n",
    "> Para este curso, **solo es estrictamente necesario utilizar padding + resize** para procesar las imágenes. Este procedimiento será suficiente para obtener el **100% del puntaje relacionado con el preprocesamiento**.\n",
    ">\n",
    "> Sin embargo, se otorgarán hasta **10% puntos extra** en la nota final a quienes:\n",
    "> - Experimenten con técnicas creativas de preprocesamiento.\n",
    "> - Logren mejoras notables en el rendimiento de los modelos gracias a dichas técnicas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb56d89",
   "metadata": {},
   "source": [
    "## Selección de Modelos Preentrenados en PyTorch\n",
    "\n",
    "Para este proyecto, los estudiantes deben seleccionar **tres modelos preentrenados** disponibles en PyTorch. Pueden elegir entre diferentes **familias de arquitecturas** (como ResNet, DenseNet o EfficientNet), o bien seleccionar diferentes **versiones dentro de una misma familia** (por ejemplo: ResNet18, ResNet50, ResNet101).\n",
    "\n",
    "### Formato de Entrada\n",
    "\n",
    "Todos los modelos preentrenados de clasificación de imágenes en PyTorch esperan entradas en el siguiente formato:\n",
    "\n",
    "- Rango de valores: **[0, 1]** o **[0, 255]** (normalizados según las estadísticas de ImageNet)\n",
    "- Normalización recomendada (para modelos preentrenados en ImageNet):\n",
    "  - **Media**: `[0.485, 0.456, 0.406]`\n",
    "  - **Desviación estándar**: `[0.229, 0.224, 0.225]`\n",
    "\n",
    "### Modelos Populares en PyTorch\n",
    "\n",
    "A continuación se presentan algunos modelos comúnmente usados, con sus características principales:\n",
    "\n",
    "| Modelo              | Tipo           | Tamaño de Entrada | # Parámetros (aprox.) |\n",
    "|---------------------|----------------|-------------------|------------------------|\n",
    "| **ResNet18**        | ResNet         | 224×224×3         | 11.7 M                 |\n",
    "| **ResNet34**        | ResNet         | 224×224×3         | 21.8 M                 |\n",
    "| **ResNet50**        | ResNet         | 224×224×3         | 25.6 M                 |\n",
    "| **ResNet101**       | ResNet         | 224×224×3         | 44.5 M                 |\n",
    "| **ResNet152**       | ResNet         | 224×224×3         | 60.2 M                 |\n",
    "| **ResNeXt50-32x4d** | ResNeXt        | 224×224×3         | 25.0 M                 |\n",
    "| **ResNeXt101-32x8d**| ResNeXt        | 224×224×3         | 88.8 M                 |\n",
    "| **WideResNet50-2**  | WideResNet     | 224×224×3         | 68.9 M                 |\n",
    "| **WideResNet101-2** | WideResNet     | 224×224×3         | 126.9 M                |\n",
    "| **DenseNet121**     | DenseNet       | 224×224×3         | 8.0 M                  |\n",
    "| **DenseNet169**     | DenseNet       | 224×224×3         | 14.1 M                 |\n",
    "| **DenseNet201**     | DenseNet       | 224×224×3         | 20.0 M                 |\n",
    "| **VGG11**           | VGG            | 224×224×3         | 132 M                  |\n",
    "| **VGG13**           | VGG            | 224×224×3         | 133 M                  |\n",
    "| **VGG16**           | VGG            | 224×224×3         | 138 M                  |\n",
    "| **VGG19**           | VGG            | 224×224×3         | 144 M                  |\n",
    "| **MobileNetV2**     | MobileNet      | 224×224×3         | 3.4 M                  |\n",
    "| **ShuffleNetV2-0.5x** | ShuffleNet   | 224×224×3         | 1.4 M                  |\n",
    "| **ShuffleNetV2-1.0x** | ShuffleNet   | 224×224×3         | 2.3 M                  |\n",
    "| **SqueezeNet1.0**   | SqueezeNet     | 224×224×3         | 1.2 M                  |\n",
    "| **SqueezeNet1.1**   | SqueezeNet     | 224×224×3         | 1.2 M                  |\n",
    "| **InceptionV3**     | Inception      | 299×299×3         | 23.9 M                 |\n",
    "| **GoogLeNet**       | GoogLeNet      | 224×224×3         | 6.6 M                  |\n",
    "| **EfficientNet-B0** | EfficientNet   | 224×224×3         | 5.3 M                  |\n",
    "| **EfficientNet-B1** | EfficientNet   | 240×240×3         | 7.8 M                  |\n",
    "| **EfficientNet-B2** | EfficientNet   | 260×260×3         | 9.2 M                  |\n",
    "| **EfficientNet-B3** | EfficientNet   | 300×300×3         | 12.2 M                 |\n",
    "| **EfficientNet-B4** | EfficientNet   | 380×380×3         | 19.3 M                 |\n",
    "| **EfficientNet-B5** | EfficientNet   | 456×456×3         | 30.4 M                 |\n",
    "| **EfficientNet-B6** | EfficientNet   | 528×528×3         | 43.0 M                 |\n",
    "| **EfficientNet-B7** | EfficientNet   | 600×600×3         | 66.0 M                 |\n",
    "| **RegNetY-4.0GF**   | RegNet         | 224×224×3         | 20.6 M                 |\n",
    "| **RegNetY-8.0GF**   | RegNet         | 224×224×3         | 39.2 M                 |\n",
    "| **RegNetY-16.0GF**  | RegNet         | 224×224×3         | 83.6 M                 |\n",
    "| **RegNetY-32.0GF**  | RegNet         | 224×224×3         | 145.0 M                |\n",
    "| **RegNetX-400MF**   | RegNet         | 224×224×3         | 5.3 M                  |\n",
    "| **RegNetX-800MF**   | RegNet         | 224×224×3         | 8.3 M                  |\n",
    "| **RegNetX-1.6GF**   | RegNet         | 224×224×3         | 11.2 M                 |\n",
    "| **RegNetX-3.2GF**   | RegNet         | 224×224×3         | 19.3 M                 |\n",
    "| **RegNetX-4.0GF**   | RegNet         | 224×224×3         | 22.7 M                 |\n",
    "| **RegNetX-8.0GF**   | RegNet         | 224×224×3         | 39.6 M                 |\n",
    "| **RegNetX-16.0GF**  | RegNet         | 224×224×3         | 83.6 M                 |\n",
    "| **RegNetX-32.0GF**  | RegNet         | 224×224×3         | 145.0 M                |\n",
    "| **ConvNeXt-Small**  | ConvNeXt       | 224×224×3         | 50.2 M                 |\n",
    "| **ConvNeXt-Base**   | ConvNeXt       | 224×224×3         | 88.6 M                 |\n",
    "| **ConvNeXt-Large**  | ConvNeXt       | 224×224×3         | 198.0 M                |\n",
    "\n",
    "\n",
    "### Requisitos del Proyecto\n",
    "\n",
    "- Seleccionar **tres modelos preentrenados** de la lista anterior o del repositorio `torchvision.models`.\n",
    "- Entrenar los tres modelos con el mismo conjunto de datos procesado.\n",
    "- Evaluar y comparar el desempeño de los modelos usando las métricas solicitadas.\n",
    "\n",
    "Pueden optar por:\n",
    "- Tres modelos **de diferente arquitectura** (como ResNet50, DenseNet121, EfficientNet-B0)\n",
    "- Tres modelos **de la misma arquitectura con distinta profundidad** (como ResNet18, ResNet50, ResNet101)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588fc6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Recomendaciones Generales\n",
    "\n",
    "A continuación se presentan algunas recomendaciones importantes para abordar este proyecto de forma eficiente:\n",
    "\n",
    "### 1. Comenzar con un modelo liviano\n",
    "\n",
    "Se recomienda comenzar el experimento utilizando un **modelo pequeño** para verificar que el flujo de entrenamiento funciona correctamente y que el modelo es capaz de aprender.\n",
    "\n",
    "Algunas opciones comunes por su bajo número de parámetros son:\n",
    "\n",
    "- `ResNet18`\n",
    "- `EfficientNet-B0`\n",
    "- `MobileNetV2`\n",
    "\n",
    "Estos modelos se mencionan únicamente como ejemplo. Cualquier modelo liviano puede usarse en esta primera etapa. Una vez validado que todo funciona, pueden considerar escalar a modelos de mayor capacidad.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Comparar modelos con parámetros similares\n",
    "\n",
    "Si se desea comparar diferentes **familias de modelos** (por ejemplo, `DenseNet` vs `ResNet` vs `EfficientNet`), es recomendable elegir modelos que tengan **cantidades similares de parámetros**, para que la comparación sea justa.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "- Comparar `ResNet50` (25.6 M) con `EfficientNet-B4` (19.3 M) es más apropiado que comparar `ResNet50` con `EfficientNet-B0` (5.3 M).\n",
    "\n",
    "\n",
    "\n",
    "### 3. Enfocarse primero en el preprocesamiento\n",
    "\n",
    "El dataset original es bastante pesado en tamaño y resolución. Por eso, es importante que al inicio del proyecto se priorice el **preprocesamiento de las imágenes**:\n",
    "\n",
    "- Aplicar **padding** para hacerlas cuadradas\n",
    "- Redimensionar a un tamaño fijo compatible con los modelos preentrenados (por ejemplo, 224x224)\n",
    "- Guardar las imágenes procesadas para evitar repetir este paso\n",
    "\n",
    "Este preprocesamiento no solo mejora el rendimiento del modelo, sino que también reduce el tamaño del conjunto de datos, lo cual facilita su manejo en plataformas como Google Colab.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Recursos computacionales y soporte\n",
    "\n",
    "En caso de encontrar dificultades con el entrenamiento debido a:\n",
    "\n",
    "- Falta de GPU\n",
    "- Memoria RAM o VRAM insuficiente\n",
    "- Problemas de almacenamiento o tiempo de ejecución\n",
    "\n",
    "Se debe notificar al profesor **lo antes posible**. No se debe esperar a último momento para comunicar estos inconvenientes, ya que podrían afectar el desarrollo del proyecto.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Criterios de Evaluación\n",
    "\n",
    "El objetivo principal de este proyecto es evaluar la capacidad para:\n",
    "\n",
    "- **Preprocesar imágenes** con padding y resize para adaptarlas al formato requerido\n",
    "- **Seleccionar e implementar un modelo preentrenado** adecuado para clasificación de imágenes\n",
    "- **Entrenar y evaluar** el modelo usando métricas estándar\n",
    "\n",
    "El desempeño del modelo no será el único criterio de evaluación. Se espera un mínimo razonable de rendimiento, con un **AUC ≥ 0.6**, lo cual ya es apenas mejor que un clasificador aleatorio.\n",
    "\n",
    "\n",
    "\n",
    "### 6. Puntos adicionales por mejoras\n",
    "\n",
    "Se otorgarán hasta un **10% extra sobre la nota del proyecto** a los grupos que:\n",
    "\n",
    "- Implementen técnicas novedosas o mejoradas de preprocesamiento\n",
    "- Optimicen el modelo o logren una mejora sustancial en rendimiento\n",
    "- Propongan soluciones bien justificadas fuera de los requerimientos mínimos\n",
    "\n",
    "\n",
    "\n",
    "### 7. Valor del proyecto\n",
    "\n",
    "Este proyecto representa el **40% de la nota final** del curso. Cumplir con los requisitos básicos descritos aquí será suficiente para obtener una buena calificación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d990d3d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Evaluación del Modelo: Métricas de Desempeño\n",
    "\n",
    "Para evaluar la calidad de los modelos de clasificación entrenados con el dataset **CODEBRIM**, se utilizarán las siguientes métricas:\n",
    "\n",
    "### 1. Accuracy (Precisión Global)\n",
    "- **Definición**: Proporción de predicciones correctas sobre el total de muestras.\n",
    "- **Modelo matemático**:  \n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. Precision (Precisión por clase)\n",
    "- **Definición**: Proporción de verdaderos positivos entre todas las predicciones positivas para una clase.\n",
    "- **Modelo matemático**:  \n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3. Recall (Sensibilidad)\n",
    "- **Definición**: Proporción de verdaderos positivos detectados correctamente entre todos los positivos reales.\n",
    "- **Modelo matemático**:  \n",
    "  $$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 4. F1-score\n",
    "- **Definición**: Media armónica entre precisión y recall. Resume ambas métricas en una sola.\n",
    "- **Modelo matemático**:  \n",
    "  $$\n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 5. Confusion Matrix (Matriz de Confusión)\n",
    "- **Descripción**: Tabla que muestra las predicciones correctas e incorrectas por clase.\n",
    "\n",
    "|                | Predicho: Defecto (1) | Predicho: Sin Defecto (0) |\n",
    "|----------------|:---------------------:|:-------------------------:|\n",
    "| **Real: Defecto (1)**     |        80              |           20              |\n",
    "| **Real: Sin Defecto (0)** |        10              |           90              |\n",
    "\n",
    "- **TP (Verdaderos Positivos):** 80  \n",
    "- **TN (Verdaderos Negativos):** 90  \n",
    "- **FP (Falsos Positivos):** 10  \n",
    "- **FN (Falsos Negativos):** 20  \n",
    "\n",
    "Esta matriz permite visualizar cuántas imágenes con defecto y sin defecto fueron correctamente o incorrectamente clasificadas por el modelo.\n",
    "\n",
    "\n",
    "\n",
    "### 6. AUC-ROC (Área bajo la curva ROC)\n",
    "- **Definición**: Representa la relación entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR).\n",
    "- **Modelo matemático**:  \n",
    "  $$\n",
    "  \\text{TPR} = \\frac{TP}{TP + FN}, \\quad\n",
    "  \\text{FPR} = \\frac{FP}{FP + TN}\n",
    "  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ef3ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Entregables\n",
    "\n",
    "- **Código fuente documentado:** debe entregarse en formato Jupyter Notebook o como scripts `.py` bien organizados y comentados.\n",
    "\n",
    "- **Repositorio en GitHub:**\n",
    "  - La entrega debe realizarse mediante un repositorio (público o privado, según se indique), que incluya:\n",
    "    - Código funcional para el entrenamiento de los tres modelos\n",
    "    - Archivo `README.md` con instrucciones claras para reproducir el entrenamiento y evaluación\n",
    "    - Presentación final en formato `.pptx` (puede incluirse también una versión en `.pdf`)\n",
    "    - Cualquier archivo o recurso adicional necesario para ejecutar el proyecto (por ejemplo, imágenes procesadas o scripts de preprocesamiento)\n",
    "\n",
    "- **Presentación oral (20 + 10 minutos):**\n",
    "  - Cada grupo dispondrá de **20 minutos de exposición** y **10 minutos para preguntas** del profesor o compañeros.\n",
    "  - La presentación debe incluir:\n",
    "    - Descripción del problema, del dataset y del preprocesamiento realizado\n",
    "    - Justificación de los modelos seleccionados\n",
    "    - Comparación visual y numérica del rendimiento de los tres modelos (AUC, F1, accuracy, etc.)\n",
    "    - Visualizaciones de resultados (por ejemplo: curvas ROC, matriz de confusión)\n",
    "    - Discusión de los hallazgos: ¿qué modelo funciona mejor? ¿por qué?\n",
    "    - Propuestas de mejora o análisis crítico sobre lo que podría mejorarse en futuras versiones\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Fecha de entrega\n",
    "\n",
    "- **Presentación oral:** lunes **11 de agosto**, durante el horario regular de clase.\n",
    "- **Entrega del código y las diapositivas (.pptx):** lunes **11 de agosto**, hasta las **23:59 (hora local)**.\n",
    "\n",
    "La entrega debe realizarse mediante un **repositorio en GitHub** que contenga:\n",
    "\n",
    "- Código funcional y documentado (notebook o scripts organizados)\n",
    "- Archivo de presentación en formato `.pptx`\n",
    "- Instrucciones breves para ejecutar el código o reproducir los resultados\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Rúbrica de Evaluación (100 puntos)\n",
    "\n",
    "### 1. Presentación oral + archivo .pptx — 50 puntos\n",
    "\n",
    "Las diapositivas presentadas y entregadas en formato `.pptx` deben reflejar claramente el trabajo realizado. Funcionan como el informe oficial del proyecto.\n",
    "\n",
    "| Criterio                                                                 | Puntos |\n",
    "|--------------------------------------------------------------------------|--------|\n",
    "| Claridad en la presentación del problema y del dataset                   | 10     |\n",
    "| Justificación del preprocesamiento y elección de modelos                 | 10     |\n",
    "| Evaluación e interpretación de resultados (AUC, accuracy, F1, etc.)      | 15     |\n",
    "| Comparación crítica entre modelos y discusión de hallazgos               | 10     |\n",
    "| Calidad visual, estructura y coherencia del archivo `.pptx`              | 5      |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Código — 50 puntos\n",
    "\n",
    "Se debe entregar el código en un notebook o script bien organizado, aunque el foco de evaluación estará más en la trazabilidad y documentación que en la complejidad técnica.\n",
    "\n",
    "| Criterio                                                                 | Puntos |\n",
    "|--------------------------------------------------------------------------|--------|\n",
    "| Código funcional para entrenamiento de los tres modelos                  | 15     |\n",
    "| Aplicación correcta del preprocesamiento requerido (padding + resize)   | 10     |\n",
    "| Evaluación del modelo con métricas adecuadas                             | 10     |\n",
    "| Estructura clara del código: secciones organizadas, comentarios, títulos | 10     |\n",
    "| Limpieza del entorno, sin código redundante o sin uso                   | 5      |\n",
    "\n",
    "---\n",
    "\n",
    "**Total: 100 puntos**\n",
    "\n",
    "> Se otorgará hasta un **10% adicional** sobre la nota del proyecto a quienes implementen técnicas avanzadas, mejoras justificadas o análisis innovadores.  \n",
    "> Este proyecto equivale al **40% de la nota final del curso**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
